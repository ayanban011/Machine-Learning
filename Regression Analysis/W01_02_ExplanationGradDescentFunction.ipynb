{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex: Explanation of the Vectorised version of our Gradient Descent function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the gradient descent function that we have vectorised taking advantage of Numpy.\n",
    "\n",
    "We use Numpy as this way we can take advantage of the parallel processing functions of our CPU to make calculations faster. As an extra bonus the function is defined in a more generic term, as we can then use matrices of any size and the function will respond with the right number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent_np(X, y, max_iterations=10000, lr=0.1, epsilon = 0.000001):\n",
    "    m, n = X.shape # number of samples, number of features\n",
    "    J = []\n",
    "\n",
    "    #initialize the weights to zero (or anything else you want, e.g. a small random value)\n",
    "    w = np.zeros(n)\n",
    "    \n",
    "    # Repeat until convergence (or max_iterations)\n",
    "    for iteration in range(max_iterations):\n",
    "        grad = np.dot(X.T , (np.dot(X, w) - y)) / m;\n",
    "        w = w - lr*grad\n",
    "        J.append(sum( (np.dot(X, w) - y)**2) / m)\n",
    "        \n",
    "        # Stopping Criterion\n",
    "        if (iteration > 2) and ( abs(J[-2] - J[-1]) < epsilon):\n",
    "            print (\"Gradient Descent converged after {} iterations\".format(iteration))\n",
    "            break\n",
    "    return w, J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me take some time to explain you the line where we calculate the gradients, as it might not be very obvious how we do this by multiplying matrices. I am talking about the line:\n",
    "\n",
    "`grad = np.dot(X.T , (np.dot(X, w) - y)) / m;`\n",
    "\n",
    "Where `X` is the input (the Design Matrix) `y` is the vector column of the output, and `w` is a vector column of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design matrix `X` for a dataset of of $m$ samples (data points) and $n$ features, should have $m$ rows and $n+1$ columns, and looks as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "x_0^{(m)} & x_1^{(m)} & \\cdots & x_n^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Each row corresponds to a data point, and each column corresponds to a feature.\n",
    "\n",
    "Our outputs (labels, correct values, ground truth) `y` is a column vector of as many rows as data points:\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The number of parameters would be equal to the number of features plus the bias (corresponding to the ficticious feature $x_0$, and we can write it as a vector `w` as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "w = \n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots   \\\\\n",
    "w_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to calculate is the gradient (partial derivatives) for all thetas from $w_0$ to $w_n$. These will be used afterwards to change each of the thetas accordingly. These are given by \n",
    "\n",
    "$\\frac{\\partial}{\\partial w_0} J(w) = {1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_0^{(i)}}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_1} J(w) = {1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_1^{(i)}}$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_n} J(w) = {1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_n^{(i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we would like at each iteration is to calculate a list of partial derivatives (gradients). This is what our vector `grad` will contain at the end of the operation. And we will do it in a single line of code, taking advantage of the linear algebra capabilities of NumPy:\n",
    "\n",
    "`grad = np.dot(X.T , (np.dot(X, w) - y)) / m;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start explaining from the inside out. First let's see what the part `np.dot(X, w)` does. This is a multiplication of the matrix `X` with the vector `w` and it gives us the $f_{w}$ for each of the points in our dataset.\n",
    "\n",
    "\\begin{equation*}\n",
    "X w = \n",
    "\\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "x_0^{(m)} & x_1^{(m)} & \\cdots & x_n^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots   \\\\\n",
    "w_n \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "f_{w}^{(1)} \\\\\n",
    "f_{w}^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "f_{w}^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the part of `np.dot(X, w) - y` is basically the residues, the differences between what we calculate and what the right value should be FOR EACH of the points of our dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Residues = \n",
    "\\begin{bmatrix}\n",
    "f_{w}^{(1)} \\\\\n",
    "f_{w}^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "f_{w}^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "f_{w}^{(1)} - y^{(1)}\\\\\n",
    "f_{w}^{(2)} - y^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "f_{w}^{(m)} - y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to calculate the gradient corresponding to a specific parameter, we need to multiply each of these residues with the corresponding feature, and then sum all these multipied residues over all the points of our dataset. This is what the other dot product does: `np.dot(X.T , (np.dot(X, w) - y))`, it transposes the Design Matrix and then multiplies it with our vector of residues. Let's see what this does:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "X^T Residues = \n",
    "\\begin{bmatrix}\n",
    "x_0^{(1)} & x_0^{(2)} & \\cdots & x_0^{(m)} \\\\\n",
    "x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(m)} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "x_n^{(1)} & x_n^{(2)} & \\cdots & x_n^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "f_{w}^{(1)} - y^{(1)}\\\\\n",
    "f_{w}^{(2)} - y^{(2)} \\\\\n",
    "\\vdots   \\\\\n",
    "f_{w}^{(m)} - y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(f_{w}^{(1)} - y^{(1)}) x_0^{(1)} + (f_{w}^{(2)} - y^{(2)}) x_0^{(2)} + \\cdots + (f_{w}^{(m)} - y^{(m)}) x_0^{(m)} \\\\\n",
    "(f_{w}^{(1)} - y^{(1)}) x_1^{(1)} + (f_{w}^{(2)} - y^{(2)}) x_1^{(2)} + \\cdots + (f_{w}^{(m)} - y^{(m)}) x_1^{(m)} \\\\\n",
    "\\vdots \\\\\n",
    "(f_{w}^{(1)} - y^{(1)}) x_1^{(m)} + (f_{w}^{(2)} - y^{(2)}) x_n^{(2)} + \\cdots + (f_{w}^{(m)} - y^{(m)}) x_n^{(m)}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_0^{(i)}} \\\\\n",
    "\\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_1^{(i)}} \\\\\n",
    "\\vdots  \\\\\n",
    "\\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_n^{(i)}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing remaining is to divide everything with m, so the final result of this line is a column vector with the following contents:\n",
    "\n",
    "`grad = np.dot(X.T , (np.dot(X, w) - y)) / m;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "grad = \n",
    "\\begin{bmatrix}\n",
    "{1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_0^{(i)}} \\\\\n",
    "{1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_1^{(i)}} \\\\\n",
    "\\vdots  \\\\\n",
    "{1 \\over m} \\sum_{i=1}^m{(f_w(x^{(i)}) - y^{(i)}) x_n^{(i)}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
